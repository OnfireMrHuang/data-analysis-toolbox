# spark-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-shell-config
  namespace: hadoop
data:
  spark-defaults.conf: |
    spark.master=k8s://https://kubernetes.default.svc
    spark.kubernetes.namespace=default
    spark.kubernetes.container.image=spark-shell-hive:3.5.3
    spark.kubernetes.container.image.pullPolicy=IfNotPresent
    spark.kubernetes.driver.pod.name=spark-shell-driver
    spark.kubernetes.authenticate.driver.serviceAccountName=spark-operator-spark
    spark.hadoop.fs.defaultFS=hdfs://hadoop-hadoop-hdfs-nn.hadoop.svc.cluster.local:9000
    spark.hadoop.hive.metastore.uris=thrift://hadoop-hadoop-hive-metastore.hadoop.svc.cluster.local:9083
    spark.sql.catalogImplementation=hive
    spark.sql.warehouse.dir=hdfs://hadoop-hadoop-hdfs-nn.hadoop.svc.cluster.local:9000/user/hive/warehouse
    spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:mysql://mysql.mysql.svc:3306/hive_metastore
    spark.hadoop.javax.jdo.option.ConnectionDriverName=com.mysql.cj.jdbc.Driver
    spark.hadoop.javax.jdo.option.ConnectionUserName=root
    spark.hadoop.javax.jdo.option.ConnectionPassword=请替换为实际密码


    